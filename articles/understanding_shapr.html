<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>`shapr`: Explaining individual machine learning predictions with Shapley values • shapr</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="`shapr`: Explaining individual machine learning predictions with Shapley values">
<meta property="og:description" content="shapr">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">shapr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.4.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/understanding_shapr.html">Explaining individual machine learning predictions with Shapley values</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
<li>
  <a href="../reference/index.html">Manual</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/NorskRegnesentral/shapr/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="understanding_shapr_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>
<code>shapr</code>: Explaining individual machine learning predictions with Shapley values</h1>
                        <h4 class="author">Camilla Lingjærde, Martin Jullum &amp; Nikolai Sellereite</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/NorskRegnesentral/shapr/blob/master/vignettes/understanding_shapr.Rmd"><code>vignettes/understanding_shapr.Rmd</code></a></small>
      <div class="hidden name"><code>understanding_shapr.Rmd</code></div>

    </div>

    
    
<blockquote>
<p><a href="#intro">Introduction</a></p>
</blockquote>
<blockquote>
<p><a href="#overview">Overview of Package</a></p>
</blockquote>
<blockquote>
<p><a href="#KSHAP">The Kernel SHAP Method</a></p>
</blockquote>
<blockquote>
<p><a href="#ex">Examples</a></p>
</blockquote>
<blockquote>
<p><a href="#advanced">Advanced usage</a></p>
</blockquote>
<blockquote>
<p><a href="#compare">Comparison to Lundberg &amp; Lee’s implementation</a></p>
</blockquote>
<p><a id="intro"></a></p>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>The <code>shapr</code> package implements an extended version of the Kernel SHAP method for approximating Shapley values (<span class="citation">Lundberg and Lee (2017)</span>), in which dependence between the features is taken into account (<span class="citation">Aas, Jullum, and Løland (2019)</span>). Estimation of Shapley values is of interest when attempting to explain complex machine learning models. Of existing work on interpreting individual predictions, Shapley values is regarded to be the only model-agnostic explanation method with a solid theoretical foundation (<span class="citation">Lundberg and Lee (2017)</span>). Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions, but it assumes independent features. <span class="citation">Aas, Jullum, and Løland (2019)</span> extend the Kernel SHAP method to handle dependent features, resulting in more accurate approximations to the true Shapley values. See the <a href="https://arxiv.org/abs/1903.10464">paper</a> (<span class="citation">Aas, Jullum, and Løland (2019)</span>) for further details.</p>
<p><a id="overview"></a></p>
<p><br></p>
</div>
<div id="overview-of-package" class="section level1">
<h1 class="hasAnchor">
<a href="#overview-of-package" class="anchor"></a>Overview of Package</h1>
<div id="functions" class="section level2">
<h2 class="hasAnchor">
<a href="#functions" class="anchor"></a>Functions</h2>
<p>Here is an overview of the main functions. You can read their documentation and see examples with <code>?function_name</code>.</p>
<table class="table">
<caption>Main functions in the <code>shapr</code> package.</caption>
<colgroup>
<col width="35%">
<col width="64%">
</colgroup>
<thead><tr class="header">
<th align="left">Function Name</th>
<th align="left">Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><code>shapr</code></td>
<td align="left">Get Shapley weights for test data.</td>
</tr>
<tr class="even">
<td align="left"><code>explain</code></td>
<td align="left">Computes kernel SHAP values for test data.</td>
</tr>
<tr class="odd">
<td align="left"><code>plot.shapr</code></td>
<td align="left">Plots the individual prediction explanations. Uses facet_wrap of ggplot.</td>
</tr>
</tbody>
</table>
<p><a id="KSHAP"></a></p>
<p><br></p>
</div>
</div>
<div id="the-kernel-shap-method" class="section level1">
<h1 class="hasAnchor">
<a href="#the-kernel-shap-method" class="anchor"></a>The Kernel SHAP Method</h1>
<p>Assume a predictive model <span class="math inline">\(f(\boldsymbol{x})\)</span> for a response value <span class="math inline">\(y\)</span> with features <span class="math inline">\(\boldsymbol{x}\in \mathbb{R}^M\)</span>, trained on a training set, and that we want to explain the predictions for new sets of data. This may be done using ideas from cooperative game theory, letting a single prediction take the place of the game being played and the features the place of the players. Letting <span class="math inline">\(N\)</span> denote the set of all <span class="math inline">\(M\)</span> players, and <span class="math inline">\(S \subseteq N\)</span> be a subset of <span class="math inline">\(|S|\)</span> players, the “contribution” function <span class="math inline">\(v(S)\)</span> describes the total expected sum of payoffs the members of <span class="math inline">\(S\)</span> can obtain by cooperation. The Shapley value (<span class="citation">Shapley (1953)</span>) is one way to distribute the total gains to the players, assuming that they all collaborate. The amount that player <span class="math inline">\(i\)</span> gets is then</p>
<p><span class="math display">\[\phi_i(v) = \phi_i = \sum_{S \subseteq N \setminus\{i\}} \frac{|S| ! (M-| S| - 1)!}{M!}(v(S\cup \{i\})-v(S)),\]</span></p>
<p>that is, a weighted mean over all subsets <span class="math inline">\(S\)</span> of players not containing player <span class="math inline">\(i\)</span>. <span class="citation">Lundberg and Lee (2017)</span> define the contribution function for a certain subset <span class="math inline">\(S\)</span> of these features <span class="math inline">\(\boldsymbol{x}_S\)</span> as <span class="math inline">\(v(S) = \mbox{E}[f(\boldsymbol{x})|\boldsymbol{x}_S]\)</span>, the expected output of the predictive model conditional on the feature values of the subset. <span class="citation">Lundberg and Lee (2017)</span> names this type of Shapley values SHAP (SHapley Additive exPlanation) values. Since the conditional expectations can be written as</p>
<p><span class="math display">\[\begin{equation}
\label{eq:CondExp}
E[f(\boldsymbol{x})|\boldsymbol{x}_s=\boldsymbol{x}_S^*] = E[f(\boldsymbol{x}_{\bar{S}},\boldsymbol{x}_S)|\boldsymbol{x}_S=\boldsymbol{x}_S^*] = 
\int f(\boldsymbol{x}_{\bar{S}},\boldsymbol{x}_S^*)\,p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)d\boldsymbol{x}_{\bar{S}},
\end{equation}\]</span></p>
<p>the conditional distributions <span class="math inline">\(p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)\)</span> are needed to compute the contributions. The Kernel SHAP method of <span class="citation">Lundberg and Lee (2017)</span> assumes feature independence, so that <span class="math inline">\(p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)=p(\boldsymbol{x}_{\bar{S}})\)</span>. If samples <span class="math inline">\(\boldsymbol{x}_{\bar{S}}^{k}, k=1,\ldots,K\)</span>, from <span class="math inline">\(p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)\)</span> are available, the conditional expectation in above can be approximated by</p>
<p><span class="math display">\[\begin{equation}
  v_{\text{KerSHAP}}(S) = \frac{1}{K}\sum_{k=1}^K f(\boldsymbol{x}_{\bar{S}}^{k},\boldsymbol{x}_S^*).
\end{equation}\]</span></p>
<p>In Kernel SHAP, <span class="math inline">\(\boldsymbol{x}_{\bar{S}}^{k}, k=1,\ldots,K\)</span> are sampled from the <span class="math inline">\(\bar{S}\)</span>-part of the training data, <em>independently</em> of <span class="math inline">\(\boldsymbol{x}_{S}\)</span>. This is motivated by using the training set as the empirical distribution of <span class="math inline">\(\boldsymbol{x}_{\bar{S}}\)</span>, and assuming that <span class="math inline">\(\boldsymbol{x}_{\bar{S}}\)</span> is independent of <span class="math inline">\(\boldsymbol{x}_S=\boldsymbol{x}_S^*\)</span>. Due to the independence assumption, if the features in a given model are highly dependent, the Kernel SHAP method may give a completely wrong answer. This can be avoided by estimating the conditional distribution <span class="math inline">\(p(\boldsymbol{x}_{\bar{S}}|\boldsymbol{x}_S=\boldsymbol{x}_S^*)\)</span> directly and generating samples from this distribution. With this small change, the contributions and Shapley values may then be approximated as in the ordinary Kernel SHAP framework. <span class="citation">Aas, Jullum, and Løland (2019)</span> propose three different approaches for estimating the conditional probabilities. The methods may also be combined, such that e.g. one method is used when conditioning on a small number of features, while another method is used otherwise.</p>
<p><a id="gaussian"></a></p>
<div id="multivariate-gaussian-distribution-approach" class="section level2">
<h2 class="hasAnchor">
<a href="#multivariate-gaussian-distribution-approach" class="anchor"></a>Multivariate Gaussian Distribution Approach</h2>
<p>The first approach arises from the assumption that the feature vector <span class="math inline">\(\boldsymbol{x}\)</span> stems from a multivariate Gaussian distribution with some mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Under this assumption, the conditional distribution <span class="math inline">\(p(\boldsymbol{x}_{\bar{\mathcal{S}}} |\boldsymbol{x}_{\mathcal{S}}=\boldsymbol{x}_{\mathcal{S}}^*)\)</span> is also multivariate Gaussian<br><span class="math inline">\(\text{N}_{|\bar{\mathcal{S}}|}(\boldsymbol{\mu}_{\bar{\mathcal{S}}|\mathcal{S}},\boldsymbol{\Sigma}_{\bar{\mathcal{S}}|\mathcal{S}})\)</span>, with analytical expressions for the conditional mean vector <span class="math inline">\(\boldsymbol{\mu}_{\bar{\mathcal{S}}|\mathcal{S}}\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}_{\bar{\mathcal{S}}|\mathcal{S}}\)</span>, see <span class="citation">Aas, Jullum, and Løland (2019)</span> for details. Hence, instead of sampling from the marginal empirical distribution of <span class="math inline">\(\boldsymbol{x}_{\bar{\mathcal{S}}}\)</span> approximated by the training data, we can sample from the Gaussian conditional distribution, which is fitted using the training data. Using the resulting samples <span class="math inline">\(\boldsymbol{x}_{\bar{\mathcal{S}}}^k, k=1,\ldots,K\)</span>, the conditional expectations be approximated as in the Kernel SHAP.</p>
<p><a id="copula"></a></p>
</div>
<div id="gaussian-copula-approach" class="section level2">
<h2 class="hasAnchor">
<a href="#gaussian-copula-approach" class="anchor"></a>Gaussian Copula Approach</h2>
<p>If the features are far from multivariate Gaussian, an alternative approach is to instead represent the marginals by their empirical distributions, and model the dependence structure by a Gaussian copula. Assuming a Gaussian copula, we may convert the marginals of the training data to Gaussian features using their empirical distributions, and then fit a multivariate Gaussian distribution to these.</p>
<p>To produce samples from the conditional distribution <span class="math inline">\(p(\boldsymbol{x}_{\bar{\mathcal{S}}} |\boldsymbol{x}_{\mathcal{S}}=\boldsymbol{x}_{\mathcal{S}}^*)\)</span>, we convert the marginals of <span class="math inline">\(\boldsymbol{x}_{\mathcal{S}}\)</span> to Gaussians, sample from the conditional Gaussian distribution as above, and convert the marginals of the samples back to the original distribution. Those samples are then used to approximate the sample from the resulting multivariate Gaussian conditional distribution. While other copulas may be used, the Gaussian copula has the benefit that we may use the analytical expressions for the conditionals <span class="math inline">\(\boldsymbol{\mu}_{\bar{\mathcal{S}}|\mathcal{S}}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{\bar{\mathcal{S}}|\mathcal{S}}\)</span>. Finally, we may convert the marginals back to their original distribution, and use the resulting samples to approximate the conditional expectations as in the Kernel SHAP.</p>
<p><a id="empirical"></a></p>
</div>
<div id="empirical-conditional-distribution-approach" class="section level2">
<h2 class="hasAnchor">
<a href="#empirical-conditional-distribution-approach" class="anchor"></a>Empirical Conditional Distribution Approach</h2>
<p>If both the dependence structure and the marginal distributions of <span class="math inline">\(\boldsymbol{x}\)</span> are very far from the Gaussian, neither of the two aforementioned methods will work very well. Few methods exists for the non-parametric estimation of conditional densities, and the classic kernel estimator (<span class="citation">Rosenblatt (1956)</span>) for non-parametric density estimation suffers greatly from the curse of dimensionality and does not provide a way to generate samples from the estimated distribution. For such situations, <span class="citation">Aas, Jullum, and Løland (2019)</span> propose an empirical conditional approach to sample approximately from <span class="math inline">\(p(\boldsymbol{x}_{\bar{\mathcal{S}}}|\boldsymbol{x}_{\mathcal{S}}^*)\)</span>. The idea is to compute weights <span class="math inline">\(w_{\mathcal{S}}(\boldsymbol{x}^*,\boldsymbol{x}^i),\ i=1,...,n_{\text{train}}\)</span> for all training instances based on their Mahalanobis distances (in the <span class="math inline">\(S\)</span> subset only) to the instance <span class="math inline">\(\boldsymbol{x}^*\)</span> to be explained. Instead of sampling from this weighted (conditional) empirical distribution, <span class="citation">Aas, Jullum, and Løland (2019)</span> suggests a more efficient variant, using only the <span class="math inline">\(K\)</span> instances with the largest weights:</p>
<p><span class="math display">\[v_{\text{condKerSHAP}}(\mathcal{S}) = \frac{\sum_{k=1}^K w_{\mathcal{S}}(\boldsymbol{x}^*,
\boldsymbol{x}^{[k]}) f(\boldsymbol{x}_{\bar{\mathcal{S}}}^{[k]},
\boldsymbol{x}_{\mathcal{S}}^*)}{\sum_{k=1}^K w_{\mathcal{S}}(\boldsymbol{x}^*,\boldsymbol{x}^{[k]})},\]</span></p>
<p>The number of samples <span class="math inline">\(K\)</span> to be used in the approximate prediction can for instance be chosen such that the <span class="math inline">\(K\)</span> largest weights accounts for a fraction <span class="math inline">\(\eta\)</span>, for example <span class="math inline">\(0.9\)</span>, of the total weight. If <span class="math inline">\(K\)</span> exceeds a certain limit, for instance <span class="math inline">\(5,000\)</span>, it might be set to that limit. A bandwidth parameter <span class="math inline">\(\sigma\)</span> used to scale the weights, must also be specified. This choice may be viewed as a bias-variance trade-off. A small <span class="math inline">\(\sigma\)</span> puts most of the weight to a few of the closest training observations and thereby gives low bias, but high variance. When <span class="math inline">\(\sigma \rightarrow \infty\)</span>, this method converges to the original Kernel SHAP assuming feature independence. Typically, when the features are highly dependent, a small <span class="math inline">\(\sigma\)</span> is typically needed such that the bias does not dominate. <span class="citation">Aas, Jullum, and Løland (2019)</span> show that a proper criterion for selecting <span class="math inline">\(\sigma\)</span> is a small-sample-size corrected version of the AIC known as AICc. As calculation of it is computationally intensive, an approximate version of the selection criterion is also suggested. Details on this is found in <span class="citation">Aas, Jullum, and Løland (2019)</span>.</p>
<p><a id="ex"></a></p>
<p><br></p>
</div>
<div id="conditional-inference-tree-approach" class="section level2">
<h2 class="hasAnchor">
<a href="#conditional-inference-tree-approach" class="anchor"></a>Conditional Inference Tree Approach</h2>
<p>The previous three methods can only handle numerical data. This means that if the data contains categorical/discrete/ordinal features, the features first have to be one-hot encoded. When the number of levels/features is large, this is not feasible. An approach that handles mixed (i.e numerical, categorical, discrete, ordinal) features and both univariate and multivariate responses is conditional inference trees (<span class="citation">Hothorn, Hornik, and Zeileis (2006)</span>).</p>
<p>Conditional inference trees is a special tree fitting procedure that relies on hypothesis tests to choose both the splitting feature and the splitting point. The tree fitting procedure is sequential: first a splitting feature is chosen (the feature that is least independent of the response), and then a splitting point is chosen for this feature. This decreases the chance of being biased towards features with many splits (<span class="citation">Hothorn, Hornik, and Zeileis (2006)</span>).</p>
<p>We use conditional inference trees (<em>ctree</em>) to model the conditional distribution, <span class="math inline">\(p(\boldsymbol{x}_{\bar{\mathcal{S}}}|\boldsymbol{x}_{\mathcal{S}}^*)\)</span>, found in the Shapley methodology. First, we fit a different conditional inference tree to each conditional distribution. Once a tree is fit for given dependent features, the end node of <span class="math inline">\(\boldsymbol{x}_{\mathcal{S}}^*\)</span> is found. Then, we sample from this end node and use the resulting samples, <span class="math inline">\(\boldsymbol{x}_{\bar{\mathcal{S}}}^k, k=1,\ldots,K\)</span>, when approximating the conditional expectations as in Kernel SHAP. See <span class="citation">Redelmeier, Jullum, and Aas (2020)</span> for more details.</p>
<p>The conditional inference trees are fit using the <em>party</em> and <em>partykit</em> packages (<span class="citation">Hothorn and Zeileis (2015)</span>).</p>
<p><a id="ex"></a></p>
<p><br></p>
</div>
</div>
<div id="examples" class="section level1">
<h1 class="hasAnchor">
<a href="#examples" class="anchor"></a>Examples</h1>
<p><code>shapr</code> supports computation of Shapley values with any predictive model which takes a set of numeric features and produces a numeric outcome. Note that the ctree method takes both numeric and categorical variables. Check under “Advanced usage” for an example of how this can be done.</p>
<p>The following example shows how a simple <code>xgboost</code> model is trained using the Boston Housing Data, and how <code>shapr</code> can be used to explain the individual predictions. Note that the empirical conditional distribution approach is the default (i.e. <code>approach = "empirical"</code>), and that the Gaussian, Gaussian copula, and ctree approaches can be used instead by setting the argument <code>approach</code> to either <code>"gaussian"</code>, <code>"copula"</code>, or <code>"ctree</code>".</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dmlc/xgboost">xgboost</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://norskregnesentral.github.io/shapr/">shapr</a></span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Boston"</span>, package <span class="op">=</span> <span class="st">"MASS"</span><span class="op">)</span>

<span class="va">x_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lstat"</span>, <span class="st">"rm"</span>, <span class="st">"dis"</span>, <span class="st">"indus"</span><span class="op">)</span>
<span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"medv"</span>

<span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="va">x_var</span><span class="op">]</span><span class="op">)</span>
<span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">Boston</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="va">y_var</span><span class="op">]</span>
<span class="va">x_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="va">x_var</span><span class="op">]</span><span class="op">)</span>

<span class="co"># Fitting a basic xgboost model to the training data</span>
<span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html">xgboost</a></span><span class="op">(</span>
  data <span class="op">=</span> <span class="va">x_train</span>,
  label <span class="op">=</span> <span class="va">y_train</span>,
  nround <span class="op">=</span> <span class="fl">20</span>,
  verbose <span class="op">=</span> <span class="cn">FALSE</span>
<span class="op">)</span>

<span class="co"># Prepare the data for explanation</span>
<span class="va">explainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/shapr.html">shapr</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">model</span><span class="op">)</span>
<span class="co">#&gt; The specified model provides feature classes that are NA. The classes of data are taken as the truth.</span>

<span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span>

<span class="co"># Computing the actual Shapley values with kernelSHAP accounting for feature dependence using</span>
<span class="co"># the empirical (conditional) distribution approach with bandwidth parameter sigma = 0.1 (default)</span>
<span class="va">explanation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="st">"empirical"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>

<span class="co"># Printing the Shapley values for the test data.</span>
<span class="co"># For more information about the interpretation of the values in the table, see ?shapr::explain.</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">explanation</span><span class="op">$</span><span class="va">dt</span><span class="op">)</span>
<span class="co">#&gt;      none     lstat         rm       dis      indus</span>
<span class="co">#&gt; 1: 22.446 5.2632030 -1.2526613 0.2920444  4.5528644</span>
<span class="co">#&gt; 2: 22.446 0.1671901 -0.7088401 0.9689005  0.3786871</span>
<span class="co">#&gt; 3: 22.446 5.9888022  5.5450858 0.5660134 -1.4304351</span>
<span class="co">#&gt; 4: 22.446 8.2142204  0.7507572 0.1893366  1.8298304</span>
<span class="co">#&gt; 5: 22.446 0.5059898  5.6875103 0.8432238  2.2471150</span>
<span class="co">#&gt; 6: 22.446 1.9929673 -3.6001958 0.8601984  3.1510531</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-2-1.png" width="672"></p>
<p>The Gaussian approach is used as follows:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the Gaussian approach</span>
<span class="va">explanation_gaussian</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="st">"gaussian"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation_gaussian</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-3-1.png" width="672"></p>
<p>The Gaussian copula approach is used as follows:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the Gaussian copula approach</span>
<span class="va">explanation_copula</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="st">"copula"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6, excluding</span>
<span class="co"># the no-covariate effect</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation_copula</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>The conditional inference tree approach is used as follows:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the conditional inference tree approach</span>
<span class="va">explanation_ctree</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="st">"ctree"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6, excluding </span>
<span class="co"># the no-covariate effect</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation_ctree</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-5-1.png" width="672"></p>
<p>We can use mixed (i.e continuous, categorical, ordinal) data with ctree. Use ctree with categorical data in the following manner:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x_var_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lstat"</span>, <span class="st">"chas"</span>, <span class="st">"rad"</span>, <span class="st">"indus"</span><span class="op">)</span>
<span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"medv"</span>

<span class="co"># convert to factors</span>
<span class="va">Boston</span><span class="op">$</span><span class="va">rad</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">$</span><span class="va">rad</span><span class="op">)</span>
<span class="va">Boston</span><span class="op">$</span><span class="va">chas</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">$</span><span class="va">chas</span><span class="op">)</span>

<span class="va">x_train_cat</span> <span class="op">&lt;-</span> <span class="va">Boston</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="va">x_var_cat</span><span class="op">]</span>
<span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">Boston</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="va">y_var</span><span class="op">]</span>
<span class="va">x_test_cat</span> <span class="op">&lt;-</span> <span class="va">Boston</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="va">x_var_cat</span><span class="op">]</span>

<span class="co"># -- special function when using categorical data + xgboost</span>
<span class="va">dummylist</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/make_dummies.html">make_dummies</a></span><span class="op">(</span>traindata <span class="op">=</span> <span class="va">x_train_cat</span>, testdata <span class="op">=</span> <span class="va">x_test_cat</span><span class="op">)</span>

<span class="va">x_train_dummy</span> <span class="op">&lt;-</span> <span class="va">dummylist</span><span class="op">$</span><span class="va">train_dummies</span>
<span class="va">x_test_dummy</span> <span class="op">&lt;-</span> <span class="va">dummylist</span><span class="op">$</span><span class="va">test_dummies</span>

<span class="co"># Fitting a basic xgboost model to the training data</span>
<span class="va">model_cat</span> <span class="op">&lt;-</span> <span class="fu">xgboost</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html">xgboost</a></span><span class="op">(</span>
  data <span class="op">=</span> <span class="va">x_train_dummy</span>,
  label <span class="op">=</span> <span class="va">y_train</span>,
  nround <span class="op">=</span> <span class="fl">20</span>,
  verbose <span class="op">=</span> <span class="cn">FALSE</span>
<span class="op">)</span>
<span class="va">model_cat</span><span class="op">$</span><span class="va">feature_list</span> <span class="op">&lt;-</span> <span class="va">dummylist</span><span class="op">$</span><span class="va">feature_list</span>

<span class="va">explainer_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/shapr.html">shapr</a></span><span class="op">(</span><span class="va">dummylist</span><span class="op">$</span><span class="va">traindata_new</span>, <span class="va">model_cat</span><span class="op">)</span>

<span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span>

<span class="va">explanation_cat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">dummylist</span><span class="op">$</span><span class="va">testdata_new</span>,
  approach <span class="op">=</span> <span class="st">"ctree"</span>,
  explainer <span class="op">=</span> <span class="va">explainer_cat</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6, excluding</span>
<span class="co"># the no-covariate effect</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation_cat</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-6-1.png" width="672"></p>
<p>We can specify parameters used to build the conditional inference trees using the following manner. Default values are based on <span class="citation">Hothorn, Hornik, and Zeileis (2006)</span>.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the conditional inference tree approach</span>
<span class="co"># We can specify parameters used to building trees by specifying mincriterion, </span>
<span class="co"># minsplit, minbucket</span>

<span class="va">explanation_ctree</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="st">"ctree"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>,
  mincriterion <span class="op">=</span> <span class="fl">0.80</span>, 
  minsplit <span class="op">=</span> <span class="fl">20</span>,
  minbucket <span class="op">=</span> <span class="fl">20</span>
<span class="op">)</span>

<span class="co"># Default parameters (based on (Hothorn, 2006)) are:</span>
<span class="co"># mincriterion = 0.95</span>
<span class="co"># minsplit = 20</span>
<span class="co"># minbucket = 7</span></code></pre></div>
<p>We can also specify multiple different mincriterion (1 minus the boundary for when to stop splitting nodes) parameters to use when conditioning on different numbers of features. In this case, a vector of length = number of features must be provided.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the conditional inference tree approach</span>
<span class="co"># Specify a vector of mincriterions instead of just one</span>
<span class="co"># In this case, when conditioning on 1 or 2 features, use mincriterion = 0.25</span>
<span class="co"># When conditioning on 3 or 4 features, use mincriterion = 0.95</span>

<span class="va">explanation_ctree</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="st">"ctree"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>,
  mincriterion <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<div id="main-arguments-in-shapr" class="section level2">
<h2 class="hasAnchor">
<a href="#main-arguments-in-shapr" class="anchor"></a>Main arguments in <code>shapr</code>
</h2>
<p>When using <code>shapr</code>, the default behavior is to use all feature combinations in the Shapley formula. Kernel SHAP’s sampling based approach may be used by specifying <code>n_combinations</code>, which is the number of feature combinations to sample. If not specified, the exact method is used. The computation time grows approximately exponentially with the number of samples. The training data and the model whose predictions we wish to explain must be provided through the arguments <code>x</code> and <code>model</code>. Note that <code>x</code> must be a <code>data.frame</code> or a <code>matrix</code>, and all elements must be finite numerical values. Currently we do not support categorical features or missing values.</p>
</div>
<div id="main-arguments-in-explain" class="section level2">
<h2 class="hasAnchor">
<a href="#main-arguments-in-explain" class="anchor"></a>Main arguments in <code>explain</code>
</h2>
<p>The test data given by <code>x</code>, whose predicted values we wish to explain, must be provided. Note that <code>x</code> must be a <code>data.frame</code> or a <code>matrix,</code> where all elements are finite numerical values. One must also provide the object returned by <code>shapr</code> through the argument <code>explainer</code>. The default approach when computing the Shapley values is the empirical approach (i.e. <code>approach = "empirical"</code>). If you’d like to use a different approach you’ll need to set <code>approach</code> equal to either <code>copula</code> or <code>gaussian</code>, or a vector of them, with length equal to the number of features. If a vector, a combined approach is used, and element <code>i</code> indicates the approach to use when conditioning on <code>i</code> variables. For more details see <a href="#combined">Combined approach</a> below.</p>
<p>When computing the kernel SHAP values by <code>explain</code>, the maximum number of samples to use in the Monte Carlo integration for every conditional expectation is controlled by the argument <code>n_samples</code> (default equals <code>1000</code>). The computation time grows approximately linear with this number. You will also need to pass a numeric value for the argument <code>prediction_zero</code>, which represents the prediction value when not conditioning on any features. We recommend setting this equal to the mean of the response, but other values, like the mean prediction of a large test data set is also a possibility. If the empirical method is used, specific settings for that approach, like a vector of fixed <span class="math inline">\(\sigma\)</span> values can be specified through the argument <code>sigma_vec</code>. See <code><a href="../reference/explain.html">?explain</a></code> for more information. If <code>approach = "gaussian"</code>, you may specify the mean vector and covariance matrix of the data generating distribution by the arguments <code>mu</code> and <code>cov_mat</code>. If not specified, they are estimated from the training data.</p>
<p><a id="advanced"></a></p>
<p><br></p>
</div>
</div>
<div id="advanced-usage" class="section level1">
<h1 class="hasAnchor">
<a href="#advanced-usage" class="anchor"></a>Advanced usage</h1>
<p><a id="combined"></a></p>
<div id="combined-approach" class="section level2">
<h2 class="hasAnchor">
<a href="#combined-approach" class="anchor"></a>Combined approach</h2>
<p>In addition to letting the user select one of the three aforementioned approaches for estimating the conditional distribution of the data (i.e. <code>approach</code> equals either <a href="#gaussian"><code>"gaussian"</code></a>, <a href="#copula"><code>"copula"</code></a>, <a href="#empirical"><code>"empirical"</code></a> or <a href="#ctree"><code>"ctree"</code></a>) the package allows the user to combine the four approaches. To simplify the usage, the flexibility is restricted such that the same approach is used when conditioning on the same number of features. This is also in line <span class="citation">Aas, Jullum, and Løland (2019, Section 3.4)</span>.</p>
<p>This can be done by setting <code>approach</code> equal to a character vector, where the length of the vector is equal to the number of features in the model. Consider a situation where you have trained a model that consists of 10 features, and you would like to use the <code>"empirical"</code> approach when you condition on 1-3 features, the <code>"copula"</code> approach when you condition on 4-5 features, and the <code>"gaussian"</code> approach when conditioning on 6 or more features. This can be applied by simply passing <code>approach = c(rep("empirical", 3), rep("copula", 2), rep("gaussian", 5))</code>, i.e. <code>approach[i]</code> determines which method to use when conditioning on <code>i</code> features.</p>
<p>The code below exemplifies this approach for a case where there are four features, using <code>"empirical", "copula"</code> and <code>"gaussian"</code> when conditioning on respectively 1, 2 and 3-4 features. Note that it does not matter what method that is specified when conditioning on all features, as that equals the actual prediction regardless of the specified approach.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the combined approach</span>
<span class="va">explanation_combined</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"empirical"</span>, <span class="st">"copula"</span>, <span class="st">"gaussian"</span>, <span class="st">"gaussian"</span><span class="op">)</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6, excluding</span>
<span class="co"># the no-covariate effect</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation_combined</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-9-1.png" width="672"></p>
<p>As a second example using <code>"ctree"</code> for the first 3 features and <code>"empirical"</code> for the last:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Use the combined approach</span>
<span class="va">explanation_combined</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test</span>,
  approach <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"ctree"</span>, <span class="st">"ctree"</span>, <span class="st">"ctree"</span>, <span class="st">"empirical"</span><span class="op">)</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span></code></pre></div>
</div>
<div id="using-ctree-when-features-are-mixed-numerical-and-categorical" class="section level2">
<h2 class="hasAnchor">
<a href="#using-ctree-when-features-are-mixed-numerical-and-categorical" class="anchor"></a>Using ctree when features are mixed numerical and categorical</h2>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lstat"</span>, <span class="st">"rm"</span>, <span class="st">"dis"</span>, <span class="st">"indus"</span><span class="op">)</span>
<span class="va">y_var</span> <span class="op">&lt;-</span> <span class="st">"medv"</span>

<span class="co"># Convert two features as factors</span>
<span class="va">dt</span> <span class="op">&lt;-</span> <span class="va">Boston</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">x_var</span>, <span class="va">y_var</span><span class="op">)</span><span class="op">]</span>
<span class="va">dt</span><span class="op">$</span><span class="va">rm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">dt</span><span class="op">$</span><span class="va">rm</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span>
<span class="va">dt</span><span class="op">$</span><span class="va">dis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">dt</span><span class="op">$</span><span class="va">dis</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span><span class="op">)</span>

<span class="va">xy_train_cat</span> <span class="op">&lt;-</span> <span class="va">dt</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="op">]</span>
<span class="va">y_train_cat</span> <span class="op">&lt;-</span> <span class="va">dt</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="va">y_var</span><span class="op">]</span>
<span class="va">x_train_cat</span> <span class="op">&lt;-</span> <span class="va">dt</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">:</span><span class="op">-</span><span class="fl">6</span>, <span class="va">x_var</span><span class="op">]</span>
<span class="va">x_test_cat</span> <span class="op">&lt;-</span> <span class="va">dt</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="va">x_var</span><span class="op">]</span>


<span class="co"># Fit a basic linear regression model to the training data</span>
<span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">lstat</span> <span class="op">+</span> <span class="va">rm</span> <span class="op">+</span> <span class="va">dis</span> <span class="op">+</span> <span class="va">indus</span>, data <span class="op">=</span> <span class="va">xy_train_cat</span><span class="op">)</span>

<span class="co"># Prepare the data for explanation</span>
<span class="va">explainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/shapr.html">shapr</a></span><span class="op">(</span><span class="va">x_train_cat</span>, <span class="va">model</span><span class="op">)</span>

<span class="co"># Specifying the phi_0, i.e. the expected prediction without any features</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y_train_cat</span><span class="op">)</span>

<span class="co"># Computing the actual Shapley values with kernelSHAP accounting for feature dependence using</span>
<span class="va">explanation_categorical</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span>
  <span class="va">x_test_cat</span>,
  approach <span class="op">=</span> <span class="st">"ctree"</span>,
  explainer <span class="op">=</span> <span class="va">explainer</span>,
  prediction_zero <span class="op">=</span> <span class="va">p</span>
<span class="op">)</span>
<span class="co"># Note that nothing has to be specified to tell "ctree" that two of the features are </span>
<span class="co"># cateogrical and two are numerical</span>

<span class="co"># Plot the resulting explanations for observations 1 and 6, excluding </span>
<span class="co"># the no-covariate effect</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation_categorical</span>, plot_phi0 <span class="op">=</span> <span class="cn">FALSE</span>, index_x_test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>
</div>
<div id="explain-custom-models" class="section level2">
<h2 class="hasAnchor">
<a href="#explain-custom-models" class="anchor"></a>Explain custom models</h2>
<p><code>shapr</code> currently natively supports explanation of predictions from models fitted with the following functions:</p>
<ul>
<li><code><a href="https://rdrr.io/r/stats/lm.html">stats::lm</a></code></li>
<li><code><a href="https://rdrr.io/r/stats/glm.html">stats::glm</a></code></li>
<li><code><a href="https://rdrr.io/pkg/ranger/man/ranger.html">ranger::ranger</a></code></li>
<li><code><a href="https://rdrr.io/pkg/mgcv/man/gam.html">mgcv::gam</a></code></li>
<li>
<code><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html">xgboost::xgboost</a></code>/<code><a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html">xgboost::xgb.train</a></code>
</li>
</ul>
<p>Any continuous response regression model or binary classification model of these model classes, can be explained with the package directly as exemplified above. Moreover, essentially any feature dependent prediction model can be explained by the package by specifying two (or one) simple additional functions to the class your model belongs to.</p>
<p><em>Note: The below procedure for specifying custom models was changed in shapr v0.2.0</em> The first class function is <code>predict_model</code>, taking the model and data (as a <code>matrix</code> or <code>data.frame/data.table</code>) as input and outputting the corresponding prediction as a numeric vector. The second (optional, but highly recommended) class function is <code>get_model_specs</code>, taking the model as input and outputting a list with the following elements: <em>labels</em> (vector with the feature names to compute Shapley values for), <em>classes</em> (a named vector with the labels as names and the class type as elements), <em>factor_levels</em> (a named list with the labels as names and vectors with the factor levels as elements (NULL if the feature is not a factor)). The <code>get_model_specs</code> function is used to check that the format of the data passed to <code>shapr</code> and <code>explain</code> have the correct format in terms of the necessary feature columns being available and having the correct class/attributes. It is highly recommended to do such checks in order to ensure correct usage of <code>shapr</code> and <code>explain</code>. If, for some reason, such checking is not desirable, one does not have to provide the <code>get_model_specs</code> function class. This will, however, throw a warning that all feature consistency checking against the model is disabled.</p>
<p>Once the above class functions are created, one can explain predictions from this model class as before. These functions <strong>can</strong> be made general enough to handle all supported model types of that class, or they can be made minimal, possibly only allowing explanation of the specific version of the model class at hand. Below we give examples of both full support versions of these functions and a minimal version which skips the <code>get_model_specs</code> function. We do this for the <code>gbm</code> model class from the <code>gbm</code> package, fitted to the same Boston data set as used above.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/gbm-developers/gbm">gbm</a></span><span class="op">)</span>
<span class="co">#&gt; Loaded gbm 2.1.8</span>

<span class="va">xy_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x_train</span>,medv <span class="op">=</span> <span class="va">y_train</span><span class="op">)</span>

<span class="va">form</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">as.formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">y_var</span>,<span class="st">"~"</span>,<span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">x_var</span>,collapse<span class="op">=</span><span class="st">"+"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Fitting a gbm model</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">825</span><span class="op">)</span>
<span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">gbm</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm</a></span><span class="op">(</span>
  <span class="va">form</span>,
  data <span class="op">=</span> <span class="va">xy_train</span>,
  distribution <span class="op">=</span> <span class="st">"gaussian"</span>
<span class="op">)</span>

<span class="co">#### Full feature versions of the three required model functions ####</span>

<span class="va">predict_model.gbm</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">newdata</span><span class="op">)</span> <span class="op">{</span>
  
  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html">requireNamespace</a></span><span class="op">(</span><span class="st">'gbm'</span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
    <span class="kw"><a href="https://rdrr.io/r/base/stop.html">stop</a></span><span class="op">(</span><span class="st">'The gbm package is required for predicting train models'</span><span class="op">)</span>
  <span class="op">}</span>

  <span class="va">model_type</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span>
    <span class="va">x</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="va">name</span> <span class="op">%in%</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"bernoulli"</span>,<span class="st">"adaboost"</span><span class="op">)</span>,
    <span class="st">"classification"</span>,
    <span class="st">"regression"</span>
  <span class="op">)</span>
  <span class="kw">if</span> <span class="op">(</span><span class="va">model_type</span> <span class="op">==</span> <span class="st">"classification"</span><span class="op">)</span> <span class="op">{</span>

    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"response"</span>,n.trees <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">n.trees</span><span class="op">)</span>
  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span>

    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span>,n.trees <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">n.trees</span><span class="op">)</span>
  <span class="op">}</span>
<span class="op">}</span>

<span class="va">get_model_specs.gbm</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span>
  <span class="va">feature_list</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span>
  <span class="va">feature_list</span><span class="op">$</span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/labels.html">labels</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">Terms</span><span class="op">)</span>
  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">feature_list</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span>

  <span class="va">feature_list</span><span class="op">$</span><span class="va">classes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">Terms</span>,<span class="st">"dataClasses"</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span>
  <span class="va">feature_list</span><span class="op">$</span><span class="va">factor_levels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/setNames.html">setNames</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span><span class="st">"list"</span>, <span class="va">m</span><span class="op">)</span>, <span class="va">feature_list</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span>
  <span class="va">feature_list</span><span class="op">$</span><span class="va">factor_levels</span><span class="op">[</span><span class="va">feature_list</span><span class="op">$</span><span class="va">classes</span><span class="op">==</span><span class="st">"factor"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span> <span class="co"># the model object doesn't contain factor levels info</span>

  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">feature_list</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Prepare the data for explanation</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="va">explainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/shapr.html">shapr</a></span><span class="op">(</span><span class="va">xy_train</span>, <span class="va">model</span><span class="op">)</span>
<span class="co">#&gt; The columns(s) medv is not used by the model and thus removed from the data.</span>
<span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">xy_train</span><span class="op">[</span>,<span class="va">y_var</span><span class="op">]</span><span class="op">)</span>
<span class="va">explanation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span><span class="va">x_test</span>, <span class="va">explainer</span>, approach <span class="op">=</span> <span class="st">"empirical"</span>, prediction_zero <span class="op">=</span> <span class="va">p0</span><span class="op">)</span>
<span class="co"># Plot results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-12-1.png" width="672"></p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="co">#### Minimal version of the three required model functions ####      </span>
<span class="co"># Note: Working only for this exact version of the model class </span>
<span class="co"># Avoiding to define get_model_specs skips all feature         </span>
<span class="co"># consistency checking between your data and model             </span>

<span class="co"># Removing the previously defined functions to simulate a fresh start</span>
<span class="fu"><a href="https://rdrr.io/r/base/rm.html">rm</a></span><span class="op">(</span><span class="va">predict_model.gbm</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/rm.html">rm</a></span><span class="op">(</span><span class="va">get_model_specs.gbm</span><span class="op">)</span>

<span class="va">predict_model.gbm</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">newdata</span><span class="op">)</span> <span class="op">{</span>
    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span>,n.trees <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">n.trees</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Prepare the data for explanation</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="va">explainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/shapr.html">shapr</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">model</span><span class="op">)</span>
<span class="co">#&gt; get_model_specs is not available for your custom model. All feature consistency checking between model and data is disabled.</span>
<span class="co">#&gt; See the 'Advanced usage' section of the vignette:</span>
<span class="co">#&gt; vignette('understanding_shapr', package = 'shapr')</span>
<span class="co">#&gt; for more information.</span>
<span class="co">#&gt; The specified model provides feature labels that are NA. The labels of data are taken as the truth.</span>
<span class="va">p0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">xy_train</span><span class="op">[</span>,<span class="va">y_var</span><span class="op">]</span><span class="op">)</span>
<span class="va">explanation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/explain.html">explain</a></span><span class="op">(</span><span class="va">x_test</span>, <span class="va">explainer</span>, approach <span class="op">=</span> <span class="st">"empirical"</span>, prediction_zero <span class="op">=</span> <span class="va">p0</span><span class="op">)</span>
<span class="co"># Plot results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">explanation</span><span class="op">)</span></code></pre></div>
<p><img src="understanding_shapr_files/figure-html/unnamed-chunk-12-2.png" width="672"></p>
<p><a id="compare"></a></p>
<p><br></p>
</div>
</div>
<div id="comparison-to-lundberg-lees-implementation" class="section level1">
<h1 class="hasAnchor">
<a href="#comparison-to-lundberg-lees-implementation" class="anchor"></a>Comparison to Lundberg &amp; Lee’s implementation</h1>
<p>As mentioned above, the original (independence assuming) Kernel SHAP implementation can be approximated by setting a large <span class="math inline">\(\sigma\)</span> value using our empirical approach. If we specify that the distances to <em>all</em> training observations should be used (i.e. setting <code>approach = "empirical"</code> and <code>w_threshold = 1</code> when using <code>explain</code>, we can approximate the original method arbitrarily well by increasing <span class="math inline">\(\sigma\)</span>. For completeness of the <code>shapr</code>, we have also implemented a version of the original method, which samples training observations independently with respect to their distances to test observations (i.e. without the large-<span class="math inline">\(\sigma\)</span> approximation). This method is available by using <code>approach = "empirical"</code> and <code>type = "independence"</code> in <code>explain</code>.</p>
<p>We have compared the results using these two variants with the original implementation of <span class="citation">Lundberg and Lee (2017)</span>, available through the Python library <a href="https://github.com/slundberg/shap"><code>shap</code></a>. As above, we used the Boston housing data, trained via <code>xgboost</code>. We specify that <em>all</em> training observations should be used when explaining all of the 6 test observations. To run the individual explanation method in the <code>shap</code> Python library we use the <code>reticulate</code> <code>R</code>-package, allowing Python code to run within <code>R</code>. As this requires installation of Python package, the comparison code and results is not included in this vignette, but can be found <a href="https://github.com/NorskRegnesentral/shapr/blob/master/inst/scripts/compare_shap_python.R">here</a>. As indicated by the (commented out) results in the file above both methods in our <code>R</code>-package give (up to numerical approximation error) identical results to the original implementation in the Python <code>shap</code> library.</p>
<p><br></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-aas2019explaining">
<p>Aas, Kjersti, Martin Jullum, and Anders Løland. 2019. “Explaining Individual Predictions When Features Are Dependent: More Accurate Approximations to Shapley Values.” <em>arXiv Preprint arXiv:1903.10464</em>.</p>
</div>
<div id="ref-hothorn2006unbiased">
<p>Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” <em>Journal of Computational and Graphical Statistics</em> 15 (3): 651–74.</p>
</div>
<div id="ref-partykit_package">
<p>Hothorn, Torsten, and Achim Zeileis. 2015. “partykit: A Modular Toolkit for Recursive Partytioning in R.” <em>Journal of Machine Learning Research</em> 16: 3905–9.</p>
</div>
<div id="ref-lundberg2017unified">
<p>Lundberg, Scott M, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In <em>Advances in Neural Information Processing Systems</em>, 4765–74.</p>
</div>
<div id="ref-Redelmeier2020ctree">
<p>Redelmeier, Annabelle, Martin Jullum, and Kjersti Aas. 2020. “Explaining Predictive Models with Mixed Features Using Shapley Values and Conditional Inference Trees.” <em>Submitted</em>.</p>
</div>
<div id="ref-rosenblatt1956">
<p>Rosenblatt, Murray. 1956. “Remarks on Some Nonparametric Estimates of a Density Function.” <em>The Annals of Mathematical Statistics</em> 27: 832–37.</p>
</div>
<div id="ref-Shapley53">
<p>Shapley, Lloyd S. 1953. “A Value for N-Person Games.” <em>Contributions to the Theory of Games</em> 2: 307–17.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Nikolai Sellereite, Martin Jullum, Annabelle Redelmeier, Norsk Regnesentral.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.9000.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
